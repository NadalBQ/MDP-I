---
title: "MDP Trabajo Clustering"
author: "Isabelle"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r librerias clustering}
library(knitr)
library(readxl)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
library(grid)
library(gridExtra)
```
# RATONES CLUSTERING

## Datos y descripción

datos = read_excel("RatonesLimpios.xlsx", col_names = TRUE)
datos = as.data.frame(datos)

newID = sapply(datos$MouseID, function (x) strsplit(x, "_")[[1]][1])
head(datos, 3)




descRatones = data.frame("variable" = colnames(datos),
                      "tipo" = c("categorical", rep("numerical", 75),
                                 rep("binary", 3), "categorical"), stringsAsFactors = FALSE)
rownames(descRatones) = descRatones$variable
descRatones


Ejecutar todo "MDP Trabajo.Rmd" para la variable datos sin anómalos

# Centrar los datos

```{r centrar}

# Apply centering ONLY to the final list of numeric columns
datos[,2:76] <- scale(datos[,2:76], center = TRUE, scale = FALSE)

# View first 3 rows
head(datos, 3)

```

## Selección de variables a utilizar y preparación de datos
El objetivo es realizar un análisis de agrupamiento de ratones con niveles de expresión de proteínas similares. Para ello, se han seleccionado las variables de proteínas para realizar el clustering.

```{r}
datos_num = datos[,descRatones$variable[descRatones$tipo == "numerical"]]
head(datos_num)
```


## Medida de distancia y tendencia de agrupamiento
Se utilizará la distancia euclídea como medida de distancia, porque se desea agrupar ratones con valores de expresión de proteínas similares y no con perfiles similares de proteínas.

```{r dist}
midist <- get_dist(datos_num, stand = FALSE, method = "euclidean")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
Se observa un gran cluster y dos clusters más pequeños.
```{r hopkins}
set.seed(100)
myN = c(20, 35, 50, 65)  # m
myhopkins = NULL
myseed = sample(1:1000, 10)
for (i in myN) {
  for (j in myseed) {
    tmp = get_clust_tendency(data = datos_num, n = i, graph = FALSE, seed = j)
    myhopkins = c(myhopkins, tmp$hopkins_stat)
  }
}
summary(myhopkins)
```
El valor del estadístico de Hopkins está alrededor del 0.9, lo que indica que los datos tienen una tendencia a agruparse al estar cerca del 1.

## Modelos jerárquicos

### Método de Ward

Obtendremos el número de clusters óptimo para el método de Ward. Vamos a combinar el análisis del coeficiente de Silhouette con la variabilidad intra-cluster para ver el número de clusters con el que quedamos. 

```{r koptJERward, fig.width=8, fig.height=4}
p1 = fviz_nbclust(x = datos_num, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = datos_num, FUNcluster = hcut, method = "wss", 
                  hc_method = "ward.D2", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```

Los resultados para el coeficiente Silhouette indican que el número óptimo de clusters es 2 clusters. Si vemos la variabilidad intra-cluster, aún está bastante alta. Si elegimos el segundo óptimo, 3 clusters, la variabilidad intra-cluster ya baja bastante y, además, parece el punto en el que se crea el codo. Si cogemos el número de clusters como 4, baja la variabilidad intra-cluster pero también baja bastante el coeficiente Silhouette. Por tanto, fijaremos el número de clusters en 3. 

Creamos a continuación los 3 clusters con el modelo jerárquico y el método de Ward. Dado que el número de observaciones lo permite, generaremos el dendrograma para visualizar la agrupación de los ratones. 


```{r ward, fig.width=8, fig.height=6, warning=FALSE}
clust1 <- hclust(midist, method="ward.D2")
grupos1 <- cutree(clust1, k=3)
table(grupos1)
fviz_dend(clust1, k = 3,
          cex = 0.5, color_labels_by_k = TRUE,
          rect = TRUE) # dibujar rectángulos
```


### Método de la media

Ahora estimaremos el número óptimo de clusters para el método de la media.


```{r koptJERmedia, fig.width=8, fig.height=4}
p1 = fviz_nbclust(x = datos_num, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "average", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = datos_num, FUNcluster = hcut, method = "wss", 
                  hc_method = "average", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```

Los resultados para el coeficiente Silhouette indican que el número óptimo de clusters es 2. Sin embargo, la suma de cuadrados intra-cluster es demasiado elevada para dos clusters. El siguiente óptimo de Silhoutte es 3 clusters, con una alta suma de cuadrados intra-cluster. Si elegimos el tercer óptimo, 4 clusters, la variabilidad intra-cluster ya baja bastante y, además, parece el punto en el que se crea el codo. Por tanto, fijaremos el número de clusters en 4. 


```{r media, fig.width=8, fig.height=6, warning=FALSE}
clust2 <- hclust(midist, method="average")
grupos2 = cutree(clust2, k = 4)
fviz_dend(clust2, k = 4,
          cex = 0.5,
          color_labels_by_k = TRUE, # colorear etiquetas por grupo
          rect = TRUE) # dibujar rectángulos
table(grupos2)
```

Sin embargo, cuando observamos el dendrograma y el número de observaciones por cluster, vemos que este método propociona clusters más desequilibrados: dos de ellos con menos de 20 observaciones, otro con 150 y otro muy heterogéneo con muchas observaciones: `r max(table(grupos2))`.

Es por ello que decidimos descartar el método de la media y compararemos el método de Ward con otros métodos de clustering no jerárquicos.

### Método Centroide

Ahora estimaremos el número óptimo de clusters para el método centroide.


```{r koptJERcentroide, fig.width=8, fig.height=4}
p1 = fviz_nbclust(x = datos_num, FUNcluster = hcut, method = "silhouette", 
                  hc_method = "centroid", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = datos_num, FUNcluster = hcut, method = "wss", 
                  hc_method = "centroid", k.max = 10, verbose = FALSE, 
                  hc_metric = "euclidean") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
```

```{r centroid, fig.width=8, fig.height=6, warning=FALSE}
clust3 <- hclust(midist, method="centroid")
grupos3 = cutree(clust3, k = 3)
fviz_dend(clust3, k = 3,
          cex = 0.5,
          color_labels_by_k = TRUE, # colorear etiquetas por grupo
          rect = TRUE) # dibujar rectángulos
table(grupos3)
```
El método de la media y el método centroide son muy similares. En este caso, el número óptimo de clusters es 3 y los clusters son más equilibrados que en el caso del método de la media. Sin embargo, el método de Ward sigue siendo mejor, ya que proporciona una mejor separación entre los clusters.

## Modelos de partición
### K-means
```{r kmeans, fig.width=8, fig.height=6}
p1 = fviz_nbclust(x = datos_num, FUNcluster = kmeans, method = "silhouette", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-means")
p2 = fviz_nbclust(x = datos_num, FUNcluster = kmeans, method = "wss", 
             k.max = 10, verbose = FALSE) +
  labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
```

```{r}
set.seed(100)
clust4 <- kmeans(datos_num, centers = 3, nstart = 20)
table(clust4$cluster)
```

```{r}
p1 = fviz_nbclust(x = datos_num, FUNcluster = pam, method = "silhouette", 
             k.max = 10, verbose = FALSE) +
  labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = datos_num, FUNcluster = pam, method = "wss", 
             k.max = 10, verbose = FALSE) +
  labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
```

```{r}
clust5 <- pam(datos_num, k = 3)
table(clust5$clustering)
```

```{r}
library(ggsci)
colores = pal_npg("nrc")(3)
par(mfrow = c(1,3))
plot(silhouette(grupos1, midist), col=colores, border=NA, main = "WARD")
plot(silhouette(clust4$cluster, midist), col=colores, border=NA, main = "K-MEDIAS")
plot(silhouette(clust5$clustering, midist), col=colores, border=NA, main = "K-MEDOIDES")
```

Observando los coeficientes de Silhouette, se elige el método de k-means con 3 clústers ya es el que tiene mayor Silhouette media y menor cantidad de observaciones con coeficientes negativos, es decir, mal clasificados.

```{r}
library(vcd)
mosaic(~ clust4$cluster + Genotipo, data = datos, shade = TRUE)
mosaic(~ clust4$cluster + Tratamiento, data = datos, shade = TRUE)
mosaic(~ clust4$cluster + Comportamiento, data = datos, shade = TRUE)
```

```{r}
datos$Cluster <- clust4$cluster

descRatonesClust = data.frame("variable" = colnames(datos),
                      "tipo" = c(rep("categorical",1),rep("numerical", 75),
                                 rep("binary", 3), rep("categorical", 3)), stringsAsFactors = FALSE)
rownames(descRatonesClust) = descRatonesClust$variable
descRatonesClust
```


```{r}
library(dplyr)
library(tidyr)
library(ggplot2)

# 1. Convertimos a formato largo
datos_long <- datos %>%
  pivot_longer(
    cols      = c("Genotipo", "Tratamiento", "Comportamiento"),   # todas las columnas bin*
    names_to  = "variable",           # nombre de la variable binaria
    values_to = "valor"               # valor 0/1
  )

# 2. Graficamos con geom_bar (stat = "count" por defecto)
ggplot(datos_long, aes(
    x    = factor(variable),           # clusters en el eje X
    fill = factor(valor)              # diferenciamos 0 vs. 1 por color
  )) +
  geom_bar(
    position = "dodge"                # barras lado a lado por valor
  ) +
  facet_wrap(~ Cluster) +            # un panel separado por cada binaria
  labs(
    x    = "Clúster",
    y    = "Número de observaciones",
    fill = "Valor binario"
  )

```

```{r}
library(dplyr)
library(ggplot2)

# 1. Conteo con dplyr
datos_counts <- datos %>%
  group_by(Cluster, Clase) %>%    # agrupa por clúster y por nivel de la variable nominal
  summarise(n = n(), .groups = "drop")  # cuenta cuántas observaciones hay en cada par

# 3. Visualización con ggplot2
ggplot(datos_counts, aes(
    x    = factor(Cluster),    # clúster en X
    y    = n,                  # conteo en Y
    fill = Clase             # nivel de la variable como color
  )) +
  geom_col(position = "dodge") +  # barras lado a lado por nivel
  labs(
    x    = "Clúster",
    y    = "Número de observaciones",
    fill = "Clases"
  ) +
  theme_minimal()

```
En el clúster 1, la mayoría de los ratones son sin down y aprendiendo y el tratamiento no afecta. el cluster 2 ratones totalmente normales y los con down con estimulacion y la droga son parecidos la expresión proteica pero todas las clases tienen cantidad parecida en el cluster 2 por lo que no es concluyente. En el clúster 3, ratones sin down y con down los dos sin estimulacion y droga y otro sin estimulacion con down y sin droga.


```{r}
res.pca_clust = PCA(datos, scale.unit = FALSE, graph = FALSE, ncp = K_sin, 
              quali.sup = which(descRatonesClust$tipo == "categorical"),
              quanti.sup = which(descRatonesClust$tipo == "binary"))
```


```{r}
fviz_pca_ind(res.pca_clust, axes = c(1,2), geom = c("point"), repel = TRUE,
             labelsize = 2,
             habillage = "Cluster", addEllipses = TRUE)
```

```{r}
fviz_pca_ind(res.pca_clust, axes = c(3,4), geom = c("point"), repel = TRUE,
             labelsize = 2,
             habillage = "Cluster", addEllipses = TRUE)
```

